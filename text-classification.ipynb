{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4f69ea-42c1-4dc0-bb68-4f58e0b9ecb3",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Based on a single text column, we want to classify each document into a single class\n",
    "\n",
    "Here we use CNN article dataset to classify each document into correct Category.\n",
    "\n",
    "Our main feature is the Description column. We have split dataset into training and validations sets in `prepare-dataset.ipynb` notebook (you can also run `prepare_dataset.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "977e9dd7-0f1a-4623-8815-e1c931073ec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:25:29.413989Z",
     "iopub.status.busy": "2022-05-06T19:25:29.411180Z",
     "iopub.status.idle": "2022-05-06T19:25:36.316828Z",
     "shell.execute_reply": "2022-05-06T19:25:36.315875Z",
     "shell.execute_reply.started": "2022-05-06T19:25:29.413944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Wiktor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Wiktor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essential modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download tokenizers and stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c5df7",
   "metadata": {},
   "source": [
    "# Simpler approaches with bag of words or tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af9814c6-9286-4c9b-a84b-d57e9cc4fb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:26:59.285403Z",
     "iopub.status.busy": "2022-05-06T19:26:59.284289Z",
     "iopub.status.idle": "2022-05-06T19:26:59.508239Z",
     "shell.execute_reply": "2022-05-06T19:26:59.506698Z",
     "shell.execute_reply.started": "2022-05-06T19:26:59.285356Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"data/training_set.csv\")\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "# the next step is to randomize the rows of the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# fetch labels\n",
    "y = df['Category'].values\n",
    "\n",
    "# initiate the kfold class from model_selection module\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# fill the new kfold column\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dcaf30",
   "metadata": {},
   "source": [
    "## bag of words + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8d617ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8757668711656442\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8819018404907976\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8742331288343558\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8726993865030674\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8711656441717791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we go over the folds created\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = linear_model.LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77005f09",
   "metadata": {},
   "source": [
    "## TfidfVectorizer + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e91b8aa1-c059-45eb-b0cf-3adcd74bbbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T18:56:16.554764Z",
     "iopub.status.busy": "2022-05-06T18:56:16.484257Z",
     "iopub.status.idle": "2022-05-06T18:56:37.171242Z",
     "shell.execute_reply": "2022-05-06T18:56:37.170740Z",
     "shell.execute_reply.started": "2022-05-06T18:56:16.554715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8619631901840491\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8803680981595092\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8665644171779141\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8726993865030674\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8542944785276073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# we go over the folds created\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = linear_model.LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dac266",
   "metadata": {},
   "source": [
    "## bag of words + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00160723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8665644171779141\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8803680981595092\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8650306748466258\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8726993865030674\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8803680981595092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# we go over the folds created\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = svm.LinearSVC()\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4abff",
   "metadata": {},
   "source": [
    "## tfidfVectorizer + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73e86371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8880368098159509\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.897239263803681\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8788343558282209\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8880368098159509\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8834355828220859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = svm.LinearSVC()\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac3650",
   "metadata": {},
   "source": [
    "## bag of words + multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "975a3ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8773006134969326\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8849693251533742\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8957055214723927\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8834355828220859\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8726993865030674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2afcf",
   "metadata": {},
   "source": [
    "## tf-idf + multinomial naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f16637a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8512269938650306\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8742331288343558\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8665644171779141\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8496932515337423\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8542944785276073\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7917f",
   "metadata": {},
   "source": [
    "# OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccd8d3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8757668711656442\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8865030674846626\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8726993865030674\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8773006134969326\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8726993865030674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = OneVsRestClassifier(linear_model.LogisticRegression(max_iter=200, n_jobs=-1))\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d8bc6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8558282208588958\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8788343558282209\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8619631901840491\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8711656441717791\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8466257668711656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = OneVsRestClassifier(linear_model.LogisticRegression(max_iter=200, n_jobs=-1))\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4abff",
   "metadata": {},
   "source": [
    "## bag of words + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33748cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['business' 'entertainment' 'health' 'news' 'politics' 'sport']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Wiktor\\code\\text-classification\\text-classification.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m XGBClassifier(eval_metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauc\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=19'>20</a>\u001b[0m \u001b[39m# fit the model on training data reviews and sentiment\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xtrain, train_df[\u001b[39m\"\u001b[39;49m\u001b[39mCategory\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=21'>22</a>\u001b[0m \u001b[39m# make predictions on test data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=22'>23</a>\u001b[0m \u001b[39m# threshold for predictions is 0.5\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wiktor/code/text-classification/text-classification.ipynb#ch0000017?line=23'>24</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(xtest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/core.py?line=529'>530</a>\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[0;32m    <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/core.py?line=530'>531</a>\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[1;32m--> <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/core.py?line=531'>532</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1357\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1351'>1352</a>\u001b[0m     expected_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1352'>1353</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1353'>1354</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m expected_classes\u001b[39m.\u001b[39mshape\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1354'>1355</a>\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m==\u001b[39m expected_classes)\u001b[39m.\u001b[39mall()\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1355'>1356</a>\u001b[0m ):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1356'>1357</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1357'>1358</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1358'>1359</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected: \u001b[39m\u001b[39m{\u001b[39;00mexpected_classes\u001b[39m}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1359'>1360</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1361'>1362</a>\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_xgb_params()\n\u001b[0;32m   <a href='file:///c%3A/Users/Wiktor/AppData/Local/Programs/Python/Python310/lib/site-packages/xgboost/sklearn.py?line=1363'>1364</a>\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got ['business' 'entertainment' 'health' 'news' 'politics' 'sport']"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = XGBClassifier(eval_metric='auc')\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4abff",
   "metadata": {},
   "source": [
    "## tfidfVectorizer + xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = XGBClassifier(eval_metric='auc')\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a45aac",
   "metadata": {},
   "source": [
    "## Best approaches out of these\n",
    "\n",
    "- tf-idf + SVM\n",
    "- bag of word + multinomial naive bayes\n",
    "- logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe085c",
   "metadata": {},
   "source": [
    "# Approaches with text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfb7e9-f24e-4a65-be3b-fce3c877668d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:38:32.422114Z",
     "iopub.status.busy": "2022-05-06T19:38:32.420717Z",
     "iopub.status.idle": "2022-05-06T19:38:32.544213Z",
     "shell.execute_reply": "2022-05-06T19:38:32.543318Z",
     "shell.execute_reply.started": "2022-05-06T19:38:32.422078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [simone, biles, withdraw, team, gymnastic, fin...\n",
       "1       [one, extraordinary, scenes, nba, brooklyn, ne...\n",
       "2       [valentino, one, greatest, charismatic, motorc...\n",
       "3       [erratically, driven, car, threatened, lives, ...\n",
       "4       [us, star, sprinter, richardson, left, roster,...\n",
       "                              ...                        \n",
       "3255    [spate, murders, feminist, djs, creating, safe...\n",
       "3256    [band, golden, black, shared, major, wins, irr...\n",
       "3257    [eileen, gu, kamila, valieva, became, teenage,...\n",
       "3258    [duchess, brought, litigation, associated, new...\n",
       "3259    [french, league, game, lyon, marseille, abando...\n",
       "Name: Description, Length: 3260, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "X_train = df['Description']\n",
    "y_train = df['Category']\n",
    "\n",
    "# we need to preprocess text: split into words, remove punctuations and stopwords\n",
    "\n",
    "transformations = [\n",
    "    lambda x: x.lower(),\n",
    "    lambda x: x.split(),\n",
    "    lambda x: [ word for word in x if word.isalpha() ],\n",
    "    lambda x: [ word for word in x if word not in stop_words ]\n",
    "]\n",
    "\n",
    "X_transformed = X_train\n",
    "\n",
    "for t in transformations:\n",
    "    X_transformed = X_transformed.apply(t)\n",
    "                   \n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af06c0b-5e86-4416-ad5d-339e3da49f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:39:37.919989Z",
     "iopub.status.busy": "2022-05-06T19:39:37.919699Z",
     "iopub.status.idle": "2022-05-06T19:39:37.964543Z",
     "shell.execute_reply": "2022-05-06T19:39:37.963395Z",
     "shell.execute_reply.started": "2022-05-06T19:39:37.919959Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7d8cd-055a-41e1-9366-f86db44bf1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea461380278e83f75cbb3c68e20eb49ee2674c5abc1db56c07c2944024ccbda8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('text-classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
