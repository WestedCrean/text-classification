{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4f69ea-42c1-4dc0-bb68-4f58e0b9ecb3",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Based on a single text column, we want to classify each document into a single class\n",
    "\n",
    "Here we use CNN article dataset to classify each document into correct Category.\n",
    "\n",
    "Our main feature is the Description column. We have split dataset into training and validations sets in `prepare-dataset.ipynb` notebook (you can also run `prepare_dataset.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977e9dd7-0f1a-4623-8815-e1c931073ec0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:25:29.413989Z",
     "iopub.status.busy": "2022-05-06T19:25:29.411180Z",
     "iopub.status.idle": "2022-05-06T19:25:36.316828Z",
     "shell.execute_reply": "2022-05-06T19:25:36.315875Z",
     "shell.execute_reply.started": "2022-05-06T19:25:29.413944Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/wiktor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/wiktor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# essential modules\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# download tokenizers and stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9814c6-9286-4c9b-a84b-d57e9cc4fb44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:26:59.285403Z",
     "iopub.status.busy": "2022-05-06T19:26:59.284289Z",
     "iopub.status.idle": "2022-05-06T19:26:59.508239Z",
     "shell.execute_reply": "2022-05-06T19:26:59.506698Z",
     "shell.execute_reply.started": "2022-05-06T19:26:59.285356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8696319018404908\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8726993865030674\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8742331288343558\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8588957055214724\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8865030674846626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv(\"data/training_set.csv\")\n",
    "\n",
    "df['kfold'] = -1\n",
    "\n",
    "# the next step is to randomize the rows of the data\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# fetch labels\n",
    "y = df['Category'].values\n",
    "\n",
    "# initiate the kfold class from model_selection module\n",
    "kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "\n",
    "# fill the new kfold column\n",
    "for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "    df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "# we go over the folds created\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    count_vec = CountVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    count_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = count_vec.transform(train_df[\"Description\"])\n",
    "    xtest = count_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = linear_model.LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91b8aa1-c059-45eb-b0cf-3adcd74bbbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T18:56:16.554764Z",
     "iopub.status.busy": "2022-05-06T18:56:16.484257Z",
     "iopub.status.idle": "2022-05-06T18:56:37.171242Z",
     "shell.execute_reply": "2022-05-06T18:56:37.170740Z",
     "shell.execute_reply.started": "2022-05-06T18:56:16.554715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Accuracy = 0.8665644171779141\n",
      "\n",
      "Fold: 1\n",
      "Accuracy = 0.8788343558282209\n",
      "\n",
      "Fold: 2\n",
      "Accuracy = 0.8634969325153374\n",
      "\n",
      "Fold: 3\n",
      "Accuracy = 0.8573619631901841\n",
      "\n",
      "Fold: 4\n",
      "Accuracy = 0.8742331288343558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# we go over the folds created\n",
    "for fold_ in range(5):\n",
    "    # temporary dataframes for train and test\n",
    "    train_df = df[df.kfold != fold_].reset_index(drop=True)\n",
    "    test_df = df[df.kfold == fold_].reset_index(drop=True)\n",
    "    \n",
    "    # initialize CountVectorizer with NLTK's word_tokenize function as tokenizer\n",
    "    tfidf_vec = TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=None\n",
    "    )\n",
    "    # fit count_vec on training data reviews\n",
    "    tfidf_vec.fit(train_df[\"Description\"])\n",
    "    # transform training and validation data reviews\n",
    "    xtrain = tfidf_vec.transform(train_df[\"Description\"])\n",
    "    xtest = tfidf_vec.transform(test_df[\"Description\"])\n",
    "    # initialize logistic regression model\n",
    "    model = linear_model.LogisticRegression(max_iter=200, n_jobs=-1)\n",
    "    # fit the model on training data reviews and sentiment\n",
    "    model.fit(xtrain, train_df[\"Category\"])\n",
    "    # make predictions on test data\n",
    "    # threshold for predictions is 0.5\n",
    "    preds = model.predict(xtest)\n",
    "    # calculate accuracy\n",
    "    accuracy = metrics.accuracy_score(test_df[\"Category\"], preds)\n",
    "    print(f\"Fold: {fold_}\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adfb7e9-f24e-4a65-be3b-fce3c877668d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:38:32.422114Z",
     "iopub.status.busy": "2022-05-06T19:38:32.420717Z",
     "iopub.status.idle": "2022-05-06T19:38:32.544213Z",
     "shell.execute_reply": "2022-05-06T19:38:32.543318Z",
     "shell.execute_reply.started": "2022-05-06T19:38:32.422078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [jersey, belonging, michael, jordan, sophomore...\n",
       "1       [born, survived, cancer, owned, worked, rice, ...\n",
       "2       [smoke, severe, wildfires, burning, west, coas...\n",
       "3       [motto, theme, bullish, barcelona, president, ...\n",
       "4       [france, speed, declassification, secret, defe...\n",
       "                              ...                        \n",
       "3255    [new, zealand, weightlifter, laurel, hubbard, ...\n",
       "3256    [turbulent, lionel, messi, atypically, slow, s...\n",
       "3257    [making, history, victory, open, earlier, coll...\n",
       "3258    [toddler, found, three, days, went, missing, p...\n",
       "3259    [lionel, messi, made, paris, debut, coming, se...\n",
       "Name: Description, Length: 3260, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "X_train = df['Description']\n",
    "y_train = df['Category']\n",
    "\n",
    "# we need to preprocess text: split into words, remove punctuations and stopwords\n",
    "\n",
    "transformations = [\n",
    "    lambda x: x.lower(),\n",
    "    lambda x: x.split(),\n",
    "    lambda x: [ word for word in x if word.isalpha() ],\n",
    "    lambda x: [ word for word in x if word not in stop_words ]\n",
    "]\n",
    "\n",
    "X_transformed = X_train\n",
    "\n",
    "for t in transformations:\n",
    "    X_transformed = X_transformed.apply(t)\n",
    "                   \n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af06c0b-5e86-4416-ad5d-339e3da49f01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T19:39:37.919989Z",
     "iopub.status.busy": "2022-05-06T19:39:37.919699Z",
     "iopub.status.idle": "2022-05-06T19:39:37.964543Z",
     "shell.execute_reply": "2022-05-06T19:39:37.963395Z",
     "shell.execute_reply.started": "2022-05-06T19:39:37.919959Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7d8cd-055a-41e1-9366-f86db44bf1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea461380278e83f75cbb3c68e20eb49ee2674c5abc1db56c07c2944024ccbda8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('text-classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
